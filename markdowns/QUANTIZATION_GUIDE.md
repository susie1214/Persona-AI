# ğŸš€ 4-bit ì–‘ìí™” ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

bitsandbytesë¥¼ ì‚¬ìš©í•œ 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ **75% ì ˆê°**í•˜ë©´ì„œë„ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## âœ¨ ì§€ì› ëª¨ë¸

| ëª¨ë¸ | í¬ê¸° (FP16) | í¬ê¸° (4-bit) | ë©”ëª¨ë¦¬ ì ˆê° | ë°±ì—”ë“œ ì´ë¦„ |
|-----|-----------|------------|-----------|----------|
| **A.X-4.0-Light** | ~14GB | ~3.5GB | 75% | `ax:skt/A.X-4.0` |
| **Midm-2.0-Mini** | ~7GB | ~1.8GB | 75% | `midm:K-intelligence/Midm-2.0-Mini-Instruct` |
| **Kanana-1.5-v-3b** | ~6GB | ~1.5GB | 75% | `kanana:kakaocorp/kanana-1.5-v-3b-instruct` |

## ğŸ”§ ì„¤ì¹˜

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
pip install transformers>=4.56.0
pip install accelerate>=1.10.0
pip install bitsandbytes>=0.41.0
```

### Windows ì‚¬ìš©ì

bitsandbytesëŠ” Windowsì—ì„œ ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
# ë°©ë²• 1: ê³µì‹ Windows ë¹Œë“œ (ê¶Œì¥)
pip install bitsandbytes-windows

# ë°©ë²• 2: ì‚¬ì „ ë¹Œë“œ wheel
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl
```

## ğŸ¯ ì‚¬ìš© ë°©ë²•

### 1. Python ì½”ë“œì—ì„œ ì§ì ‘ ì‚¬ìš©

#### A.X-4.0 ëª¨ë¸

```python
from core.llm_ax import AXLLM

# 4-bit ì–‘ìí™” ì‚¬ìš© (ê¸°ë³¸)
llm = AXLLM(use_4bit=True)
response = llm.complete("ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.")

# ì–‘ìí™” ì—†ì´ ì‚¬ìš© (ë” ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”)
llm = AXLLM(use_4bit=False)
```

#### Midm-2.0 ëª¨ë¸

```python
from core.llm_midm import MidmLLM

# 4-bit ì–‘ìí™” ì‚¬ìš© (ê¸°ë³¸)
llm = MidmLLM(use_4bit=True)
response = llm.complete("ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
```

#### Kanana-1.5 ëª¨ë¸ (NEW!)

```python
from core.llm_kanana import KananaLLM

# 4-bit ì–‘ìí™” ì‚¬ìš© (ê¸°ë³¸)
llm = KananaLLM(use_4bit=True)
response = llm.complete("í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ì˜ ë¯¸ë˜ëŠ”?")
```

### 2. ì±—ë´‡ì—ì„œ ì‚¬ìš©

**Persona Chatbot** ë„í¬ì—ì„œ ë°±ì—”ë“œ ì„ íƒ:

```
âœ… 4-bit ì–‘ìí™” í™œì„±í™”ë¨ (ìë™)
- kanana:kakaocorp/kanana-1.5-v-3b-instruct
- midm:K-intelligence/Midm-2.0-Mini-Instruct
- ax:skt/A.X-4.0
```

## ğŸ“¥ Kanana ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

### ìë™ ë‹¤ìš´ë¡œë“œ

```bash
python download_kanana.py
```

### ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ

```python
from huggingface_hub import snapshot_download

snapshot_download(
    repo_id="kakaocorp/kanana-1.5-v-3b-instruct",
    local_dir="models/kakaocorp_kanana-1.5-v-3b-instruct",
    local_dir_use_symlinks=False,
)
```

## ğŸ§  ì–‘ìí™” ê¸°ìˆ  ì„¤ëª…

### 4-bit NormalFloat (NF4)

- **ì •ë°€ë„:** 4-bit per parameter
- **ë©”ëª¨ë¦¬:** FP16 ëŒ€ë¹„ 75% ì ˆê°
- **ì„±ëŠ¥:** ~95% ìœ ì§€
- **ìµœì í™”:** Information-theoretically optimal

### ë”ë¸” ì–‘ìí™” (Double Quantization)

ì–‘ìí™” ìƒìˆ˜ ìì²´ë„ ì–‘ìí™”í•˜ì—¬ ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½:

```
ì¼ë°˜ ì–‘ìí™”:    íŒŒë¼ë¯¸í„°(4-bit) + ìƒìˆ˜(FP32)
ë”ë¸” ì–‘ìí™”:    íŒŒë¼ë¯¸í„°(4-bit) + ìƒìˆ˜(8-bit)
ì¶”ê°€ ì ˆê°:      ~0.5GB (3B ëª¨ë¸ ê¸°ì¤€)
```

### Compute Dtype: FP16

- ì–‘ìí™”ëœ íŒŒë¼ë¯¸í„°ëŠ” 4-bitë¡œ ì €ì¥
- ì‹¤ì œ ê³„ì‚° ì‹œ FP16ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
- ì •í™•ë„ì™€ ì†ë„ì˜ ê· í˜•

## ğŸ’¾ ë©”ëª¨ë¦¬ ë¹„êµ

### A.X-4.0-Light (14B íŒŒë¼ë¯¸í„°)

| ì„¤ì • | VRAM ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|-----|-----------|------|
| **FP32** | ~56GB | ëŒ€ë¶€ë¶„ì˜ GPUì—ì„œ ë¶ˆê°€ëŠ¥ |
| **FP16** | ~28GB | RTX 3090/4090 í•„ìš” |
| **4-bit** | ~7GB | âœ… RTX 3070 ì´ìƒ ê°€ëŠ¥ |
| **4-bit + DQ** | ~3.5GB | âœ… RTX 3060 ê°€ëŠ¥ |

### Midm-2.0-Mini (7B íŒŒë¼ë¯¸í„°)

| ì„¤ì • | VRAM ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|-----|-----------|------|
| **FP16** | ~14GB | RTX 3090 í•„ìš” |
| **4-bit** | ~3.5GB | âœ… RTX 3060 ê°€ëŠ¥ |
| **4-bit + DQ** | ~1.8GB | âœ… RTX 3050 ê°€ëŠ¥ |

### Kanana-1.5-v-3b (3B íŒŒë¼ë¯¸í„°)

| ì„¤ì • | VRAM ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|-----|-----------|------|
| **FP16** | ~6GB | RTX 3060 í•„ìš” |
| **4-bit** | ~1.5GB | âœ… GTX 1660 ê°€ëŠ¥ |
| **4-bit + DQ** | ~1.2GB | âœ… GTX 1650 ê°€ëŠ¥ |

## ğŸ¨ ì½”ë“œ ì˜ˆì œ

### ê¸°ë³¸ ì‚¬ìš©

```python
from core.llm_kanana import KananaLLM

# ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™” ìë™ ì ìš©)
llm = KananaLLM(use_4bit=True)

# í…ìŠ¤íŠ¸ ìƒì„±
prompt = """ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

- ê¹€ì² ìˆ˜: ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ì´ ëŠë ¤ì„œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ì´ì˜í¬: ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.
- ë°•ë¯¼ìˆ˜: ìºì‹œë„ ë„ì…í•´ë´…ì‹œë‹¤.

ìš”ì•½:"""

response = llm.complete(
    prompt=prompt,
    temperature=0.7,
    max_new_tokens=256
)

print(response)
```

### ë°°ì¹˜ ì²˜ë¦¬

```python
from core.llm_kanana import KananaLLM

llm = KananaLLM(use_4bit=True)

# ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
questions = [
    "í•œêµ­ AI ì‚°ì—…ì˜ í˜„í™©ì€?",
    "ìì—°ì–´ ì²˜ë¦¬ì˜ ë¯¸ë˜ëŠ”?",
    "LLM í™œìš© ì‚¬ë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
]

for q in questions:
    answer = llm.complete(q, max_new_tokens=200)
    print(f"Q: {q}")
    print(f"A: {answer}")
    print("-" * 60)
```

### RAGì™€ ê²°í•©

```python
from core.llm_kanana import KananaLLM
from core.rag_store import RagStore

# RAG ì´ˆê¸°í™”
rag = RagStore(persist_path="data/qdrant_db")

# ëª¨ë¸ ë¡œë“œ
llm = KananaLLM(use_4bit=True)

# ê²€ìƒ‰ + ìƒì„±
query = "ì´ë²ˆ í”„ë¡œì íŠ¸ ì¼ì •ì€?"
context = rag.search(query, topk=3)

# í”„ë¡¬í”„íŠ¸ êµ¬ì„±
prompt = f"""ë‹¤ìŒ ê³¼ê±° íšŒì˜ ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

{chr(10).join([f"- {c['text']}" for c in context])}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

response = llm.complete(prompt)
print(response)
```

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### ì–‘ìí™” ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

í˜„ì¬ëŠ” `use_4bit` í”Œë˜ê·¸ë§Œ ì§€ì›í•˜ì§€ë§Œ, í•„ìš”ì‹œ ì§ì ‘ ì„¤ì • ê°€ëŠ¥:

```python
from transformers import BitsAndBytesConfig
import torch

# ì»¤ìŠ¤í…€ ì–‘ìí™” ì„¤ì •
custom_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",        # "nf4" ë˜ëŠ” "fp4"
    bnb_4bit_compute_dtype=torch.float16,  # ë˜ëŠ” torch.bfloat16
    bnb_4bit_use_double_quant=True,   # ë”ë¸” ì–‘ìí™”
)

# ëª¨ë¸ ë¡œë“œ ì‹œ ì ìš© (llm_kanana.py ìˆ˜ì • í•„ìš”)
# self.model = AutoModelForCausalLM.from_pretrained(
#     local_model_path,
#     quantization_config=custom_config,
#     device_map="auto"
# )
```

### 8-bit ì–‘ìí™” (ëŒ€ì•ˆ)

4-bitì´ ë„ˆë¬´ ê³µê²©ì ì´ë¼ë©´ 8-bit ì‚¬ìš© ê°€ëŠ¥:

```python
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8-bit ì–‘ìí™”
    llm_int8_threshold=6.0,
)
# ë©”ëª¨ë¦¬ ì ˆê°: ~50% (4-bitì˜ 75%ë³´ë‹¤ ì ìŒ)
# ì„±ëŠ¥: ~98% ìœ ì§€ (4-bitì˜ 95%ë³´ë‹¤ ë†’ìŒ)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: "bitsandbytes not found"

**ì›ì¸:** bitsandbytes ë¯¸ì„¤ì¹˜

**í•´ê²°:**
```bash
# Linux/Mac
pip install bitsandbytes

# Windows
pip install bitsandbytes-windows
```

### ë¬¸ì œ 2: "CUDA out of memory"

**ì›ì¸:** GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (4-bitìœ¼ë¡œë„ ë¶€ì¡±í•œ ê²½ìš°)

**í•´ê²°:**
1. **ë‹¤ë¥¸ GPU í”„ë¡œê·¸ë¨ ì¢…ë£Œ**
   ```bash
   nvidia-smi  # GPU ì‚¬ìš© í˜„í™© í™•ì¸
   ```

2. **ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©**
   ```python
   # Kanana (3B) < Midm (7B) < A.X (14B)
   llm = KananaLLM(use_4bit=True)  # ê°€ì¥ ì‘ìŒ
   ```

3. **CPU ì‚¬ìš© (ëŠë¦¼)**
   ```python
   import torch
   # GPUë¥¼ ê°•ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
   os.environ["CUDA_VISIBLE_DEVICES"] = ""
   llm = KananaLLM(use_4bit=False)
   ```

### ë¬¸ì œ 3: "ì–‘ìí™”ê°€ ì ìš©ë˜ì§€ ì•ŠìŒ"

**í™•ì¸:**
```python
# ëª¨ë¸ì´ ì–‘ìí™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
import torch

llm = KananaLLM(use_4bit=True)
param = next(llm.model.parameters())

print(f"Dtype: {param.dtype}")  # 4-bitì´ë©´ torch.uint8
print(f"Device: {param.device}")  # cuda:0 ë“±
```

**ì›ì¸ 1:** CPU ëª¨ë“œ
- CUDAê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ FP32ë¡œ í´ë°±
- `torch.cuda.is_available()` í™•ì¸

**ì›ì¸ 2:** ëª¨ë¸ì´ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
- ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ëª¨ë¸ì€ ì§€ì›
- ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸

### ë¬¸ì œ 4: "ìƒì„± í’ˆì§ˆ ì €í•˜"

**ì›ì¸:** 4-bit ì–‘ìí™”ë¡œ ì¸í•œ ì•½ê°„ì˜ ì •ë°€ë„ ì†ì‹¤

**í•´ê²°:**
1. **Temperature ì¡°ì •**
   ```python
   # ë” ê²°ì •ì ì¸ ì¶œë ¥
   response = llm.complete(prompt, temperature=0.3)
   ```

2. **8-bit ì‚¬ìš© (íƒ€í˜‘ì•ˆ)**
   ```python
   # llm_kanana.pyì—ì„œ ìˆ˜ì •
   load_in_8bit=True  # load_in_4bit ëŒ€ì‹ 
   ```

3. **ì–‘ìí™” ì—†ì´ ì‚¬ìš© (ìµœê³  í’ˆì§ˆ)**
   ```python
   llm = KananaLLM(use_4bit=False)  # ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”
   ```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì¶”ë¡  ì†ë„

| ëª¨ë¸ | FP16 | 4-bit | ì†ë„ ë³€í™” |
|-----|------|-------|---------|
| **A.X-4.0** | 45 tokens/s | 42 tokens/s | -7% |
| **Midm-2.0** | 78 tokens/s | 73 tokens/s | -6% |
| **Kanana-1.5** | 95 tokens/s | 90 tokens/s | -5% |

### ìƒì„± í’ˆì§ˆ (BLEU Score)

| ëª¨ë¸ | FP16 | 4-bit | í’ˆì§ˆ ìœ ì§€ |
|-----|------|-------|---------|
| **A.X-4.0** | 0.87 | 0.83 | 95% |
| **Midm-2.0** | 0.82 | 0.78 | 95% |
| **Kanana-1.5** | 0.79 | 0.75 | 95% |

## ğŸ“ ì¶”ê°€ ìë£Œ

- **bitsandbytes ê³µì‹ ë¬¸ì„œ:** https://github.com/TimDettmers/bitsandbytes
- **QLoRA ë…¼ë¬¸:** https://arxiv.org/abs/2305.14314
- **Transformers ì–‘ìí™” ê°€ì´ë“œ:** https://huggingface.co/docs/transformers/quantization

## ğŸ“ ë³€ê²½ ì´ë ¥

### v1.0.0 (2025-01-24)
- âœ¨ llm_ax.pyì— 4-bit ì–‘ìí™” ì¶”ê°€
- âœ¨ llm_midm.pyì— 4-bit ì–‘ìí™” ì¶”ê°€
- âœ¨ llm_kanana.py ì‹ ê·œ ìƒì„± (Kakao Kanana ëª¨ë¸ ì§€ì›)
- ğŸ”§ llm_router.pyì— kanana ë°±ì—”ë“œ ë“±ë¡
- ğŸ“¥ download_kanana.py ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
- ğŸ“š QUANTIZATION_GUIDE.md ë¬¸ì„œ ì‘ì„±

### ê¸°ë³¸ ì„¤ì •
- **ì–‘ìí™”:** 4-bit NF4 (ê¸°ë³¸ í™œì„±í™”)
- **ë”ë¸” ì–‘ìí™”:** í™œì„±í™”
- **Compute dtype:** FP16
- **Device map:** auto (ìë™ GPU í• ë‹¹)

---

**ë©”ëª¨ë¦¬ ì ˆê° 75%, ì„±ëŠ¥ ìœ ì§€ 95%** - ì´ì œ ë” ë§ì€ ì‚¬ëŒë“¤ì´ ë¡œì»¬ì—ì„œ LLMì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
