import os
import librosa
import numpy as np
from datetime import timedelta, datetime
import yaml
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.audio.core.model import Model
from faster_whisper import WhisperModel
from dataclasses import dataclass
from pathlib import Path
import torch
import pyaudio
import wave
import threading
import queue
import tempfile
from collections import deque
import time
import json
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
import uuid
import atexit
import pickle
from sklearn.metrics.pairwise import cosine_similarity

@dataclass
class SpeakerProfile:
    """í™”ì í”„ë¡œí•„ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    speaker_id: str  # ì‹¤ì œ í™”ì ID (Person_A, Person_B ë“±)
    embeddings: List[List[float]]  # í™”ì ì„ë² ë”© ë²¡í„°ë“¤
    last_seen: float  # ë§ˆì§€ë§‰ ë“±ì¥ ì‹œê°„
    total_duration: float  # ì´ ë°œí™” ì‹œê°„
    sample_count: int  # ìƒ˜í”Œ ìˆ˜

@dataclass
class ConversationEntry:
    """ëŒ€í™” ì—”íŠ¸ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    timestamp: str
    speaker: str
    text: str
    start_time: float
    end_time: float
    embedding: List[float] = None

@dataclass
class Config:
    """Configuration for real-time diarization and transcription."""
    PYANNOTE_MODEL_PATH: Path = Path("D:/Persona-AI/models/diart_model")
    WHISPER_MODEL_PATH: Path = Path("D:/Persona-AI/models/whisper-small-ct2")
    
    WHISPER_DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"
    WHISPER_COMPUTE_TYPE: str = "float16" if torch.cuda.is_available() else "int8"
    WHISPER_LANG: str = "ko"
    
    # Audio recording settings
    SAMPLE_RATE: int = 16000
    CHUNK_SIZE: int = 1024
    CHANNELS: int = 1
    AUDIO_FORMAT: int = pyaudio.paInt16
    
    # Processing settings
    BUFFER_DURATION: float = 30.0  # 30ì´ˆ ë²„í¼
    PROCESS_INTERVAL: float = 10.0  # 10ì´ˆë§ˆë‹¤ ì²˜ë¦¬
    MIN_SEG_DUR: float = 0.35
    OVERLAP_DURATION: float = 5.0  # ê²¹ì¹˜ëŠ” êµ¬ê°„ (ì—°ì†ì„± ë³´ì¥)
    
    # Storage settings
    OUTPUT_DIR: Path = Path("./output")
    QDRANT_PATH: Path = Path("./qdrant_storage")  # ë¡œì»¬ íŒŒì¼ ì €ì¥ ê²½ë¡œ
    SPEAKER_PROFILES_PATH: Path = Path("./speaker_profiles.pkl")  # í™”ì í”„ë¡œí•„ ì €ì¥
    COLLECTION_NAME: str = "conversation_embeddings"
    EMBEDDING_MODEL: str = "dragonkue/BGE-m3-ko"  # í•œêµ­ì–´ íŠ¹í™” BGE ëª¨ë¸
    
    # Speaker continuity settings
    SPEAKER_SIMILARITY_THRESHOLD: float = 0.75  # í™”ì ìœ ì‚¬ë„ ì„ê³„ê°’
    MAX_SPEAKER_EMBEDDINGS: int = 10  # í™”ìë‹¹ ìµœëŒ€ ë³´ê´€í•  ì„ë² ë”© ìˆ˜
    SPEAKER_TIMEOUT: float = 3600.0  # í™”ì ì •ë³´ ìœ ì§€ ì‹œê°„ (1ì‹œê°„)

class RealTimeDiarization:
    def __init__(self, config: Config):
        self.config = config
        self.audio_queue = queue.Queue()
        self.audio_buffer = deque(maxlen=int(config.BUFFER_DURATION * config.SAMPLE_RATE))
        self.is_recording = False
        self.audio_stream = None
        self.pyaudio_instance = None
        
        # í™”ì ì—°ì†ì„± ê´€ë¦¬
        self.speaker_profiles: Dict[str, SpeakerProfile] = {}  # ì‹¤ì œ í™”ì í”„ë¡œí•„
        self.session_speaker_mapping: Dict[str, str] = {}  # pyannote ID -> ì‹¤ì œ í™”ì ID ë§¤í•‘
        self.load_speaker_profiles()  # ì´ì „ ì„¸ì…˜ì˜ í™”ì ì •ë³´ ë¡œë“œ
        
        # ëŒ€í™” ë‚´ìš© ì €ì¥ìš©
        self.conversation_log: List[ConversationEntry] = []
        self.session_id = str(uuid.uuid4())
        self.start_time = datetime.now()
        
        # ëª¨ë¸ ë¡œë“œ
        print("[INFO] Loading models...")
        self.diar_pipeline = self.load_diarization_pipeline()
        self.whisper_model = self.load_whisper_model()
        self.embedding_model = self.load_embedding_model()
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.qdrant_client = self.initialize_qdrant()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.config.OUTPUT_DIR.mkdir(exist_ok=True)
        
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìë™ ì €ì¥
        atexit.register(self.save_conversation)
        
    def hhmmss(self, seconds: float) -> str:
        """Converts seconds to HH:MM:SS format."""
        return str(timedelta(seconds=round(seconds)))
    
    def load_diarization_pipeline(self) -> SpeakerDiarization:
        """Loads the pyannote speaker diarization pipeline."""
        print("[INFO] Loading local Pyannote model...")
        segmentation_model = Model.from_pretrained(
            self.config.PYANNOTE_MODEL_PATH / "segmentation-3.0" / "pytorch_model.bin"
        )
        embedding_model = Model.from_pretrained(
            self.config.PYANNOTE_MODEL_PATH / "wespeaker-voxceleb-resnet34-LM" / "pytorch_model.bin"
        )

        with open(self.config.PYANNOTE_MODEL_PATH / "config.yaml", "r") as f:
            pipeline_config = yaml.safe_load(f)

        diar_pipeline = SpeakerDiarization(
            segmentation=segmentation_model,
            embedding=embedding_model,
        )
        diar_pipeline.instantiate(pipeline_config['params'])
        return diar_pipeline

    def load_whisper_model(self) -> WhisperModel:
        """Loads the faster-whisper model."""
        print("[INFO] Loading Whisper model...")
        return WhisperModel(
            str(self.config.WHISPER_MODEL_PATH),
            device=self.config.WHISPER_DEVICE,
            compute_type=self.config.WHISPER_COMPUTE_TYPE
        )
    
    def load_embedding_model(self) -> SentenceTransformer:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        print("[INFO] Loading embedding model...")
        return SentenceTransformer(self.config.EMBEDDING_MODEL)
    
    def initialize_qdrant(self) -> QdrantClient:
        """Qdrant ë¡œì»¬ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±"""
        print("[INFO] Initializing Qdrant local storage...")
        try:
            # ë¡œì»¬ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            self.config.QDRANT_PATH.mkdir(exist_ok=True)
            
            # ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = QdrantClient(
                path=str(self.config.QDRANT_PATH)
            )
            
            # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            collections = client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if self.config.COLLECTION_NAME not in collection_names:
                # ì„ë² ë”© ì°¨ì› í™•ì¸ (í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±)
                test_embedding = self.embedding_model.encode(["test"])
                embedding_dim = len(test_embedding[0])
                
                print(f"[INFO] Creating Qdrant collection with dimension {embedding_dim}")
                client.create_collection(
                    collection_name=self.config.COLLECTION_NAME,
                    vectors_config=VectorParams(
                        size=embedding_dim,
                        distance=Distance.COSINE
                    )
                )
            else:
                print(f"[INFO] Using existing Qdrant collection: {self.config.COLLECTION_NAME}")
            
            print(f"[INFO] Qdrant local storage initialized at: {self.config.QDRANT_PATH}")
            return client
            
        except Exception as e:
            print(f"[ERROR] Failed to initialize Qdrant local storage: {e}")
            print("[WARNING] Continuing without Qdrant. Data will only be saved as JSON.")
            return None
    
    def load_speaker_profiles(self):
        """ì´ì „ ì„¸ì…˜ì˜ í™”ì í”„ë¡œí•„ ë¡œë“œ"""
        try:
            if self.config.SPEAKER_PROFILES_PATH.exists():
                with open(self.config.SPEAKER_PROFILES_PATH, 'rb') as f:
                    self.speaker_profiles = pickle.load(f)
                
                # ì˜¤ë˜ëœ í™”ì ì •ë³´ ì •ë¦¬
                current_time = time.time()
                expired_speakers = []
                
                for speaker_id, profile in self.speaker_profiles.items():
                    if current_time - profile.last_seen > self.config.SPEAKER_TIMEOUT:
                        expired_speakers.append(speaker_id)
                
                for speaker_id in expired_speakers:
                    del self.speaker_profiles[speaker_id]
                
                print(f"ğŸ“‹ ì´ì „ ì„¸ì…˜ í™”ì {len(self.speaker_profiles)}ëª… ë¡œë“œì™„ë£Œ")
                if expired_speakers:
                    print(f"ğŸ§¹ ë§Œë£Œëœ í™”ì {len(expired_speakers)}ëª… ì •ë¦¬ì™„ë£Œ")
                
        except Exception as e:
            print(f"[WARNING] í™”ì í”„ë¡œí•„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.speaker_profiles = {}
    
    def save_speaker_profiles(self):
        """í™”ì í”„ë¡œí•„ì„ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(self.config.SPEAKER_PROFILES_PATH, 'wb') as f:
                pickle.dump(self.speaker_profiles, f)
            print(f"ğŸ’¾ í™”ì í”„ë¡œí•„ ì €ì¥ì™„ë£Œ: {len(self.speaker_profiles)}ëª…")
        except Exception as e:
            print(f"âŒ í™”ì í”„ë¡œí•„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_speaker_embedding(self, audio_segment: np.ndarray) -> Optional[List[float]]:
        """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ í™”ì ì„ë² ë”© ì¶”ì¶œ"""
        try:
            # ì„ì‹œ íŒŒì¼ ìƒì„±í•˜ì—¬ í™”ì ì„ë² ë”© ì¶”ì¶œ
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                with wave.open(tmp_file.name, 'wb') as wav_file:
                    wav_file.setnchannels(self.config.CHANNELS)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(self.config.SAMPLE_RATE)
                    audio_int16 = (audio_segment * 32767).astype(np.int16)
                    wav_file.writeframes(audio_int16.tobytes())
                
                # pyannote ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì¬ì‚¬ìš©)
                embedding_result = self.diar_pipeline._embedding(tmp_file.name)
                if hasattr(embedding_result, 'data') and len(embedding_result.data) > 0:
                    # í‰ê·  ì„ë² ë”© ê³„ì‚°
                    speaker_embedding = np.mean(embedding_result.data, axis=0)
                    os.unlink(tmp_file.name)
                    return speaker_embedding.tolist()
                
                os.unlink(tmp_file.name)
                return None
                
        except Exception as e:
            print(f"[WARNING] í™”ì ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def identify_speaker(self, pyannote_speaker: str, audio_segment: np.ndarray, segment_duration: float) -> str:
        """pyannote í™”ì IDë¥¼ ì‹¤ì œ í™”ì IDë¡œ ë§¤í•‘"""
        current_time = time.time()
        
        # ì´ë¯¸ ë§¤í•‘ëœ í™”ìì¸ ê²½ìš°
        if pyannote_speaker in self.session_speaker_mapping:
            mapped_speaker = self.session_speaker_mapping[pyannote_speaker]
            
            # í™”ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            if mapped_speaker in self.speaker_profiles:
                profile = self.speaker_profiles[mapped_speaker]
                profile.last_seen = current_time
                profile.total_duration += segment_duration
                
                # ìƒˆë¡œìš´ ì„ë² ë”© ì¶”ê°€ (ê°€ë”ì”©)
                if profile.sample_count % 5 == 0:  # 5ë²ˆë§ˆë‹¤ í•œ ë²ˆì”© ì„ë² ë”© ì—…ë°ì´íŠ¸
                    speaker_embedding = self.get_speaker_embedding(audio_segment)
                    if speaker_embedding:
                        profile.embeddings.append(speaker_embedding)
                        if len(profile.embeddings) > self.config.MAX_SPEAKER_EMBEDDINGS:
                            profile.embeddings.pop(0)  # ì˜¤ë˜ëœ ê²ƒ ì œê±°
                
                profile.sample_count += 1
            
            return mapped_speaker
        
        # ìƒˆë¡œìš´ pyannote í™”ì - ê¸°ì¡´ í™”ìì™€ ë§¤ì¹­ ì‹œë„
        speaker_embedding = self.get_speaker_embedding(audio_segment)
        if not speaker_embedding:
            # ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ ID í• ë‹¹
            temp_speaker_id = f"Speaker_{len(self.session_speaker_mapping) + 1}"
            self.session_speaker_mapping[pyannote_speaker] = temp_speaker_id
            return temp_speaker_id
        
        best_match_speaker = None
        best_similarity = 0.0
        
        # ê¸°ì¡´ í™”ìë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
        for speaker_id, profile in self.speaker_profiles.items():
            if not profile.embeddings:
                continue
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for stored_embedding in profile.embeddings:
                similarity = cosine_similarity([speaker_embedding], [stored_embedding])[0][0]
                similarities.append(similarity)
            
            avg_similarity = np.mean(similarities)
            if avg_similarity > best_similarity:
                best_similarity = avg_similarity
                best_match_speaker = speaker_id
        
        # ì„ê³„ê°’ ì´ìƒì˜ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ í™”ìê°€ ìˆëŠ” ê²½ìš°
        if best_match_speaker and best_similarity >= self.config.SPEAKER_SIMILARITY_THRESHOLD:
            real_speaker_id = best_match_speaker
            print(f"ğŸ”— í™”ì ì—°ê²°: {pyannote_speaker} â†’ {real_speaker_id} (ìœ ì‚¬ë„: {best_similarity:.3f})")
        else:
            # ìƒˆë¡œìš´ í™”ì ìƒì„±
            real_speaker_id = f"Person_{chr(65 + len([s for s in self.speaker_profiles.keys() if s.startswith('Person_')]))}"
            print(f"âœ¨ ìƒˆ í™”ì ë“±ë¡: {pyannote_speaker} â†’ {real_speaker_id}")
        
        # ë§¤í•‘ ì €ì¥
        self.session_speaker_mapping[pyannote_speaker] = real_speaker_id
        
        # í™”ì í”„ë¡œí•„ ìƒì„±/ì—…ë°ì´íŠ¸
        if real_speaker_id not in self.speaker_profiles:
            self.speaker_profiles[real_speaker_id] = SpeakerProfile(
                speaker_id=real_speaker_id,
                embeddings=[speaker_embedding],
                last_seen=current_time,
                total_duration=segment_duration,
                sample_count=1
            )
        else:
            profile = self.speaker_profiles[real_speaker_id]
            profile.embeddings.append(speaker_embedding)
            if len(profile.embeddings) > self.config.MAX_SPEAKER_EMBEDDINGS:
                profile.embeddings.pop(0)
            profile.last_seen = current_time
            profile.total_duration += segment_duration
            profile.sample_count += 1
        
        return real_speaker_id
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """PyAudio callback for audio input."""
        if status:
            print(f"[WARNING] Audio callback status: {status}")
        
        # Convert bytes to numpy array
        audio_data = np.frombuffer(in_data, dtype=np.int16)
        # Normalize to float32 (-1.0 to 1.0)
        audio_data = audio_data.astype(np.float32) / 32768.0
        
        # Add to buffer
        self.audio_buffer.extend(audio_data)
        
        # Add to queue for processing
        self.audio_queue.put(audio_data.copy())
        
        return (in_data, pyaudio.paContinue)
    
    def start_recording(self):
        """Start recording from microphone."""
        self.pyaudio_instance = pyaudio.PyAudio()
        
        print("[INFO] Available audio devices:")
        for i in range(self.pyaudio_instance.get_device_count()):
            info = self.pyaudio_instance.get_device_info_by_index(i)
            print(f"  {i}: {info['name']} (channels: {info['maxInputChannels']})")
        
        try:
            self.audio_stream = self.pyaudio_instance.open(
                format=self.config.AUDIO_FORMAT,
                channels=self.config.CHANNELS,
                rate=self.config.SAMPLE_RATE,
                input=True,
                frames_per_buffer=self.config.CHUNK_SIZE,
                stream_callback=self.audio_callback
            )
            
            self.is_recording = True
            self.audio_stream.start_stream()
            print("ğŸ™ï¸  Recording started. Press Ctrl+C to stop.")
            print("ğŸ”Š Listening for speech...")
            print("=" * 80)
            
        except Exception as e:
            print(f"[ERROR] Failed to start recording: {e}")
            self.cleanup()
    
    def process_audio_segment(self, audio_data: np.ndarray, base_time: float):
        """Process audio segment for diarization and transcription."""
        try:
            # Create temporary WAV file
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                # Write audio data to temporary file
                with wave.open(tmp_file.name, 'wb') as wav_file:
                    wav_file.setnchannels(self.config.CHANNELS)
                    wav_file.setsampwidth(2)  # 16-bit
                    wav_file.setframerate(self.config.SAMPLE_RATE)
                    
                    # Convert float32 back to int16 for wave file
                    audio_int16 = (audio_data * 32767).astype(np.int16)
                    wav_file.writeframes(audio_int16.tobytes())
                
                tmp_file_path = tmp_file.name
            
            # Perform diarization
            segment_duration = len(audio_data)/self.config.SAMPLE_RATE
            print(f"ğŸ”„ Processing audio segment ({segment_duration:.1f}s)...")
            diar_result = self.diar_pipeline(tmp_file_path)
            
            # Transcribe each speaker segment
            current_time = datetime.now()
            for turn, _, pyannote_speaker in diar_result.itertracks(yield_label=True):
                start_time = base_time + turn.start
                end_time = base_time + turn.end
                
                if turn.end - turn.start < self.config.MIN_SEG_DUR:
                    continue
                
                # Extract segment from audio data
                i0 = max(0, int(turn.start * self.config.SAMPLE_RATE))
                i1 = min(len(audio_data), int(turn.end * self.config.SAMPLE_RATE))
                segment_wav = audio_data[i0:i1]
                
                if segment_wav.size == 0:
                    continue
                
                # ì‹¤ì œ í™”ì ID ì‹ë³„
                real_speaker = self.identify_speaker(pyannote_speaker, segment_wav, turn.end - turn.start)
                
                # Transcribe segment
                segments, _ = self.whisper_model.transcribe(
                    segment_wav,
                    language=self.config.WHISPER_LANG,
                    vad_filter=True,
                    beam_size=1,
                    word_timestamps=False
                )
                
                text = " ".join([s.text.strip() for s in segments if s.text])
                
                if text.strip():
                    timestamp = current_time.strftime("%H:%M:%S")
                    duration = end_time - start_time
                    
                    # ì‹¤ì‹œê°„ STT ê²°ê³¼ í„°ë¯¸ë„ ì¶œë ¥ (ì‹¤ì œ í™”ì ID ì‚¬ìš©)
                    print("=" * 80)
                    print(f"ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ê²°ê³¼")
                    print(f"â° ì‹œê°„: {timestamp}")
                    print(f"ğŸ‘¤ í™”ì: {real_speaker}")
                    if pyannote_speaker != real_speaker:
                        print(f"ğŸ”— ì›ë³¸ ID: {pyannote_speaker}")
                    print(f"â±ï¸  ì§€ì†ì‹œê°„: {duration:.2f}ì´ˆ")
                    print(f"ğŸ’¬ ë‚´ìš©: {text}")
                    
                    # ëŒ€í™” ë¡œê·¸ì— ì¶”ê°€ (ì‹¤ì œ í™”ì ID ì‚¬ìš©)
                    entry = ConversationEntry(
                        id=str(uuid.uuid4()),
                        timestamp=current_time.isoformat(),
                        speaker=real_speaker,
                        text=text.strip(),
                        start_time=start_time,
                        end_time=end_time
                    )
                    self.conversation_log.append(entry)
                    
                    # ì¦‰ì‹œ Qdrantì— ì €ì¥
                    self.save_to_qdrant_realtime(entry)
                    
                    print("=" * 80)
                    print()  # ë¹ˆ ì¤„ ì¶”ê°€
            
            # Clean up temporary file
            try:
                os.unlink(tmp_file_path)
            except:
                pass
                
        except Exception as e:
            print(f"[ERROR] Processing error: {e}")
    
    def processing_loop(self):
        """Main processing loop that runs in a separate thread."""
        process_buffer = []
        last_process_time = time.time()
        
        while self.is_recording:
            try:
                # Collect audio data
                try:
                    audio_chunk = self.audio_queue.get(timeout=0.1)
                    process_buffer.extend(audio_chunk)
                except queue.Empty:
                    continue
                
                current_time = time.time()
                
                # Process when buffer is large enough or enough time has passed
                buffer_duration = len(process_buffer) / self.config.SAMPLE_RATE
                
                if (buffer_duration >= self.config.PROCESS_INTERVAL or 
                    current_time - last_process_time >= self.config.PROCESS_INTERVAL):
                    
                    if len(process_buffer) > 0:
                        audio_array = np.array(process_buffer, dtype=np.float32)
                        
                        # Calculate base time for timestamps
                        base_time = current_time - buffer_duration
                        
                        # Process in separate thread to avoid blocking
                        processing_thread = threading.Thread(
                            target=self.process_audio_segment,
                            args=(audio_array, base_time)
                        )
                        processing_thread.daemon = True
                        processing_thread.start()
                        
                        # Keep some overlap for continuity
                        overlap_samples = int(self.config.OVERLAP_DURATION * self.config.SAMPLE_RATE)
                        if len(process_buffer) > overlap_samples:
                            process_buffer = process_buffer[-overlap_samples:]
                        else:
                            process_buffer = []
                        
                        last_process_time = current_time
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"[ERROR] Processing loop error: {e}")
    
    def save_conversation(self):
        """ëŒ€í™” ë‚´ìš©ì„ JSONìœ¼ë¡œ ì €ì¥ (QdrantëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ë¯¸ ì €ì¥ë¨)"""
        if not self.conversation_log:
            print("[INFO] No conversation data to save.")
            return
        
        print(f"\nğŸ’¾ JSON íŒŒì¼ë¡œ ëŒ€í™” ì €ì¥ ì¤‘... ({len(self.conversation_log)}ê°œ í•­ëª©)")
        
        try:
            # JSON ì €ì¥ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            conversation_data = {
                "session_id": self.session_id,
                "start_time": self.start_time.isoformat(),
                "end_time": datetime.now().isoformat(),
                "total_entries": len(self.conversation_log),
                "conversation": []
            }
            
            for entry in self.conversation_log:
                # JSONìš© ë°ì´í„° (ì„ë² ë”©ì€ ì œì™¸)
                entry_dict = {
                    "id": entry.id,
                    "timestamp": entry.timestamp,
                    "speaker": entry.speaker,
                    "text": entry.text,
                    "start_time": entry.start_time,
                    "end_time": entry.end_time,
                    "duration": entry.end_time - entry.start_time
                }
                conversation_data["conversation"].append(entry_dict)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            json_filename = self.config.OUTPUT_DIR / f"conversation_{self.session_id[:8]}_{self.start_time.strftime('%Y%m%d_%H%M%S')}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_filename}")
            
            # ì„¸ì…˜ ì¢…ë£Œ í†µê³„ ì¶œë ¥
            if self.qdrant_client:
                speakers = set(entry.speaker for entry in self.conversation_log)
                total_duration = sum(entry.end_time - entry.start_time for entry in self.conversation_log)
                
                print("\n" + "=" * 60)
                print("ğŸ“Š ì„¸ì…˜ í†µê³„")
                print("=" * 60)
                print(f"ğŸ‘¥ ê°ì§€ëœ í™”ì: {', '.join(sorted(speakers))}")
                print(f"â±ï¸  ì´ ë°œí™” ì‹œê°„: {total_duration:.2f}ì´ˆ")
                print(f"ğŸ’¾ Qdrantì— ì €ì¥ëœ í•­ëª©: {len(self.conversation_log)}ê°œ")
                print(f"ğŸ“ JSON íŒŒì¼: {json_filename.name}")
                print(f"ğŸ‘¥ í™”ì í”„ë¡œí•„: {len(self.speaker_profiles)}ëª… ì €ì¥")
                
                # í™”ìë³„ í†µê³„
                for speaker_id, profile in self.speaker_profiles.items():
                    print(f"   - {speaker_id}: {profile.total_duration:.1f}ì´ˆ ({profile.sample_count}íšŒ ë°œí™”)")
                
                print("=" * 60)
            
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def search_conversation(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Qdrantì—ì„œ ìœ ì‚¬í•œ ëŒ€í™” ë‚´ìš© ê²€ìƒ‰"""
        if not self.qdrant_client:
            print("[ERROR] Qdrant client not available.")
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_model.encode([query])[0].tolist()
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_results = self.qdrant_client.search(
                collection_name=self.config.COLLECTION_NAME,
                query_vector=query_embedding,
                limit=limit,
                with_payload=True
            )
            
            results = []
            for result in search_results:
                results.append({
                    "id": result.id,
                    "score": result.score,
                    "speaker": result.payload["speaker"],
                    "text": result.payload["text"],
                    "timestamp": result.payload["timestamp"],
                    "session_id": result.payload["session_id"]
                })
            
            return results
            
        except Exception as e:
            print(f"[ERROR] Search failed: {e}")
            return []
    
    def save_to_qdrant_realtime(self, entry: ConversationEntry):
        """ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¨ì¼ ì—”íŠ¸ë¦¬ë¥¼ Qdrantì— ì €ì¥"""
        if not self.qdrant_client:
            return
        
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_model.encode([entry.text])[0].tolist()
            entry.embedding = embedding
            
            # Qdrant í¬ì¸íŠ¸ ìƒì„±
            point = PointStruct(
                id=entry.id,
                vector=embedding,
                payload={
                    "session_id": self.session_id,
                    "timestamp": entry.timestamp,
                    "speaker": entry.speaker,
                    "text": entry.text,
                    "start_time": entry.start_time,
                    "end_time": entry.end_time,
                    "duration": entry.end_time - entry.start_time
                }
            )
            
            # Qdrantì— ì¦‰ì‹œ ì €ì¥
            self.qdrant_client.upsert(
                collection_name=self.config.COLLECTION_NAME,
                points=[point]
            )
            
            print(f"ğŸ’¾ Qdrant ì €ì¥ ì™„ë£Œ (ID: {entry.id[:8]}...)")
            
        except Exception as e:
            print(f"âŒ Qdrant ì‹¤ì‹œê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def run(self):
        """Main run method."""
        try:
            self.start_recording()
            
            # Start processing thread
            processing_thread = threading.Thread(target=self.processing_loop)
            processing_thread.daemon = True
            processing_thread.start()
            
            # Keep main thread alive
            while self.is_recording:
                try:
                    time.sleep(0.1)
                except KeyboardInterrupt:
                    print("\nğŸ›‘ ì‚¬ìš©ìê°€ ì¤‘ì§€ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤...")
                    break
                    
        except Exception as e:
            print(f"[ERROR] Runtime error: {e}")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """Clean up resources."""
        print("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.is_recording = False
        
        if self.audio_stream:
            self.audio_stream.stop_stream()
            self.audio_stream.close()
        
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()
        
        # ëŒ€í™” ë‚´ìš© ì €ì¥
        print("ğŸ“ ìµœì¢… JSON íŒŒì¼ ì €ì¥ ì¤‘...")
        self.save_conversation()
        
        # í™”ì í”„ë¡œí•„ ì €ì¥
        self.save_speaker_profiles()
        
        print("âœ… ì •ë¦¬ ì™„ë£Œ!")

def main():
    """Main function."""
    config = Config()
    
    print("ğŸ¯ ì‹¤ì‹œê°„ í™”ì ë¶„ë¦¬ ë° ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬:", config.OUTPUT_DIR)
    print("ğŸ’¾ Qdrant ë¡œì»¬ ì €ì¥ì†Œ:", config.QDRANT_PATH)
    print("ğŸ¤– ì„ë² ë”© ëª¨ë¸:", config.EMBEDDING_MODEL)
    print("=" * 60)
    print("ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    
    try:
        rt_diarization = RealTimeDiarization(config)
        rt_diarization.run()
    except Exception as e:
        print(f"âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜¤ë¥˜: {e}")
    
    print("ğŸ‘‹ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()