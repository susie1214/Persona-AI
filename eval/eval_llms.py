# -*- coding: utf-8 -*-
"""
eval/eval_llms.py
- í”„ë¡œë¸Œ(prompts) JSONLì„ ì½ì–´ 3ê°œ ëª¨ë¸(OpenAI / skt A.X / Midm)ì„ ì‹¤í–‰/ìºì‹œ
- ëª¨ë¸ ì¶œë ¥ â†’ ë¼ë²¨ ë§¤í•‘(í‚¤ì›Œë“œ ê¸°ë°˜) â†’ Precision/Recall/F1 ê³„ì‚°(ë§ˆì´í¬ë¡œ/ë§¤í¬ë¡œ)
- ê²°ê³¼ë¥¼ ì½˜ì†” í‘œ + CSVë¡œ ì €ì¥

ì…ë ¥:
  data/prompts.jsonl           # [{"id": "q1", "prompt": "...", "gold": ["LABEL_A","LABEL_B"]}, ...]
  eval/labels.yaml             # ë¼ë²¨â†’í‚¤ì›Œë“œ ë§¤í•‘ (ëª¨ë¸ ì¶œë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¼ë²¨ë¡œ ë³€í™˜)
ì¶œë ¥:
  predictions/{model}.jsonl    # ëª¨ë¸ë³„ ì˜ˆì¸¡ ìºì‹œ
  eval_reports/report_<ts>.csv  # ìµœì¢… ë¦¬í¬íŠ¸

ì‚¬ìš© ì˜ˆ:
  (venv) python -m eval.eval_llms --mode all
  (venv) python -m eval.eval_llms --mode predict
  (venv) python -m eval.eval_llms --mode eval
"""
import os, json, argparse, datetime, csv, sys, re
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple

try:
    import yaml
except Exception as e:
    print("[INFO] PyYAML ë¯¸ì„¤ì¹˜ â†’ ì„¤ì¹˜ í•„ìš”: pip install pyyaml"); raise

# ë³¸ í”„ë¡œì íŠ¸ ë˜í¼
try:
    from core.llm_openai import OpenAILLM
    from core.llm_ax import AXLLM
    from core.llm_midm import MidmLLM
except Exception as e:
    print("[ERROR] core.* LLM ë˜í¼ import ì‹¤íŒ¨. í”„ë¡œì íŠ¸ êµ¬ì¡°/ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”:", e)
    raise

# ----------------------------- IO -----------------------------
ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data"
EVAL_DIR = ROOT / "eval"
PROMPTS_PATH = DATA_DIR / "prompts.jsonl"
LABELS_YAML = EVAL_DIR / "labels.yaml"
PRED_DIR = ROOT / "predictions"
REPORT_DIR = ROOT / "eval_reports"
PRED_DIR.mkdir(exist_ok=True, parents=True)
REPORT_DIR.mkdir(exist_ok=True, parents=True)

MODELS = {
    "OpenAI": lambda: OpenAILLM("gpt-4o-mini"),
    "A.X":    lambda: AXLLM("skt/A.X-4.0"),
    "Midm":   lambda: MidmLLM("K-intelligence/Midm-2.0-Mini-Instruct"),
}

# ---------------------- helpers: file read/write ----------------------
def read_jsonl(path: Path) -> List[Dict[str, Any]]:
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            items.append(json.loads(line))
    return items

def write_jsonl(path: Path, items: List[Dict[str, Any]]):
    with open(path, "w", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")

def load_labels_yaml(path: Path) -> Dict[str, List[str]]:
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}
    # to lowercase keyword list
    return {lbl: [kw.lower() for kw in kws] for lbl, kws in data.items()}

# ---------------------- labeling: text -> labels ----------------------
def text_to_labels(text: str, label_kws: Dict[str, List[str]]) -> Set[str]:
    """ëª¨ë¸ ì¶œë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¼ë²¨ ì„¸íŠ¸ë¡œ ë³€í™˜ (í‚¤ì›Œë“œ OR ë§¤ì¹­, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)."""
    t = (text or "").lower()
    found = set()
    for lbl, kws in label_kws.items():
        if any(kw in t for kw in kws):
            found.add(lbl)
    return found

# ---------------------- metrics ----------------------
def f1_micro_macro(all_gold: List[Set[str]], all_pred: List[Set[str]]) -> Dict[str, float]:
    """ë©€í‹°ë¼ë²¨ ë§ˆì´í¬ë¡œ/ë§¤í¬ë¡œ P/R/F1."""
    labels = sorted(list({l for s in all_gold for l in s} | {l for s in all_pred for l in s}))
    # per-label stats
    per = {}
    for L in labels:
        tp = sum(1 for g, p in zip(all_gold, all_pred) if (L in g and L in p))
        fp = sum(1 for g, p in zip(all_gold, all_pred) if (L not in g and L in p))
        fn = sum(1 for g, p in zip(all_gold, all_pred) if (L in g and L not in p))
        prec = tp / (tp + fp) if (tp + fp) else 0.0
        rec  = tp / (tp + fn) if (tp + fn) else 0.0
        f1   = (2*prec*rec)/(prec+rec) if (prec+rec) else 0.0
        per[L] = {"tp": tp, "fp": fp, "fn": fn, "precision": prec, "recall": rec, "f1": f1}

    # micro
    TP = sum(v["tp"] for v in per.values())
    FP = sum(v["fp"] for v in per.values())
    FN = sum(v["fn"] for v in per.values())
    micro_p = TP / (TP + FP) if (TP + FP) else 0.0
    micro_r = TP / (TP + FN) if (TP + FN) else 0.0
    micro_f1 = (2*micro_p*micro_r)/(micro_p+micro_r) if (micro_p+micro_r) else 0.0

    # macro
    macro_p = sum(v["precision"] for v in per.values())/len(per) if per else 0.0
    macro_r = sum(v["recall"] for v in per.values())/len(per) if per else 0.0
    macro_f1 = sum(v["f1"] for v in per.values())/len(per) if per else 0.0

    return {
        "micro_precision": micro_p, "micro_recall": micro_r, "micro_f1": micro_f1,
        "macro_precision": macro_p, "macro_recall": macro_r, "macro_f1": macro_f1,
        "per_label": per,
        "labels": labels,
    }

# ---------------------- prediction ----------------------
def predict_model(model_name: str, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ê° ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìºì‹œë¥¼ ë°˜í™˜ [{id, prompt, gold, pred_text}]"""
    print(f"\n[RUN] {model_name} ì˜ˆì¸¡ ìƒì„± ì¤‘...")
    runner = MODELS[model_name]()
    outs = []
    for it in items:
        pid = it["id"]
        prompt = it["prompt"]
        try:
            text = runner.complete(prompt, temperature=0.2)
        except Exception as e:
            text = f"[ERROR] {e}"
        outs.append({"id": pid, "prompt": prompt, "gold": it.get("gold", []), "pred_text": text})
    return outs

def ensure_predictions(mode: str) -> Dict[str, List[Dict[str, Any]]]:
    """modeì— ë”°ë¼ (predict/eval/all) predictions/{model}.jsonl ìƒì„± ë˜ëŠ” ë¡œë“œ."""
    items = read_jsonl(PROMPTS_PATH)
    preds = {}
    for m in MODELS.keys():
        f = PRED_DIR / f"{m}.jsonl"
        if mode in ("all", "predict") or not f.exists():
            outs = predict_model(m, items)
            write_jsonl(f, outs)
        preds[m] = read_jsonl(f)
    return preds

# ---------------------- evaluation ----------------------
def evaluate(preds: Dict[str, List[Dict[str, Any]]], label_kws: Dict[str, List[str]]):
    results = []
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    report_csv = REPORT_DIR / f"report_{ts}.csv"

    print("\n===================== EVAL REPORT =====================")
    for model_name, rows in preds.items():
        gold_sets = [set(r.get("gold", [])) for r in rows]
        pred_sets = [text_to_labels(r.get("pred_text", ""), label_kws) for r in rows]

        met = f1_micro_macro(gold_sets, pred_sets)
        micro = (met["micro_precision"], met["micro_recall"], met["micro_f1"])
        macro = (met["macro_precision"], met["macro_recall"], met["macro_f1"])

        # ì½˜ì†” ì¶œë ¥
        print(f"\n[MODEL] {model_name}")
        print(f"  Micro  P/R/F1: {micro[0]:.4f} / {micro[1]:.4f} / {micro[2]:.4f}")
        print(f"  Macro  P/R/F1: {macro[0]:.4f} / {macro[1]:.4f} / {macro[2]:.4f}")
        print("  Per-label:")
        for L in met["labels"]:
            s = met["per_label"][L]
            print(f"    - {L:>12s} | P {s['precision']:.3f}  R {s['recall']:.3f}  F1 {s['f1']:.3f}  (TP{ s['tp']}, FP{ s['fp']}, FN{ s['fn']})")

        # CSV ì¶•ì 
        results.append({
            "model": model_name,
            "micro_precision": f"{micro[0]:.6f}",
            "micro_recall":    f"{micro[1]:.6f}",
            "micro_f1":        f"{micro[2]:.6f}",
            "macro_precision": f"{macro[0]:.6f}",
            "macro_recall":    f"{macro[1]:.6f}",
            "macro_f1":        f"{macro[2]:.6f}",
        })

    # CSV ì €ì¥
    with open(report_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=list(results[0].keys()))
        w.writeheader()
        for r in results: w.writerow(r)

    print("\nğŸ“„ CSV ì €ì¥:", report_csv)
    print("=======================================================\n")

# ---------------------- main ----------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", default="all", choices=["all","predict","eval"],
                    help="all: ì˜ˆì¸¡+í‰ê°€ / predict: ì˜ˆì¸¡ë§Œ / eval: ìºì‹œë¡œ í‰ê°€ë§Œ")
    args = ap.parse_args()

    if not PROMPTS_PATH.exists():
        print(f"[ERROR] {PROMPTS_PATH} ê°€ ì—†ìŠµë‹ˆë‹¤. data/prompts.jsonl ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
    if not LABELS_YAML.exists():
        print(f"[ERROR] {LABELS_YAML} ê°€ ì—†ìŠµë‹ˆë‹¤. eval/labels.yaml ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    label_kws = load_labels_yaml(LABELS_YAML)

    if args.mode in ("all", "predict"):
        preds = ensure_predictions(mode="all" if args.mode=="all" else "predict")
    else:
        # eval only: ê¸°ì¡´ ìºì‹œ í•„ìš”
        preds = {m: read_jsonl(PRED_DIR / f"{m}.jsonl") for m in MODELS.keys()}
        for m, rows in preds.items():
            if not rows:
                print(f"[ERROR] predictions/{m}.jsonl ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. ë¨¼ì € --mode predict ì‹¤í–‰í•˜ì„¸ìš”.")
                sys.exit(1)

    evaluate(preds, label_kws)

if __name__ == "__main__":
    main()
