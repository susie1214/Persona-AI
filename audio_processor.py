# audio_processor.py
# ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ìŒì„± ì¸ì‹ ëª¨ë“ˆ

import os
import io
import time
import tempfile
import threading
import queue
from collections import deque
from typing import Optional, List, Dict, Callable
import numpy as np
import soundfile as sf
import pickle
import wave
from sklearn.metrics.pairwise import cosine_similarity
from PyQt6.QtCore import QObject, pyqtSignal

from config import config
from models import Segment, SpeakerProfile, ConversationEntry

# Runtime imports with error handling
try:
    import pyaudio
except ImportError:
    pyaudio = None

try:
    from faster_whisper import WhisperModel
except ImportError:
    WhisperModel = None

try:
    from pyannote.audio import Pipeline as PyannotePipeline
except ImportError:
    PyannotePipeline = None

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None

class SpeakerManager:
    """í™”ì ê´€ë¦¬ ë° ì‹ë³„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.speaker_profiles: Dict[str, SpeakerProfile] = {}
        self.session_speaker_mapping: Dict[str, str] = {}
        self.load_speaker_profiles()
    
    def load_speaker_profiles(self):
        """ì €ì¥ëœ í™”ì í”„ë¡œí•„ ë¡œë“œ"""
        try:
            if config.storage.SPEAKER_PROFILES_PATH.exists():
                with open(config.storage.SPEAKER_PROFILES_PATH, 'rb') as f:
                    self.speaker_profiles = pickle.load(f)
                
                # ì˜¤ë˜ëœ í™”ì ì •ë³´ ì •ë¦¬
                current_time = time.time()
                expired_speakers = []
                
                for speaker_id, profile in self.speaker_profiles.items():
                    if current_time - profile.last_seen > config.speaker.SPEAKER_TIMEOUT:
                        expired_speakers.append(speaker_id)
                
                for speaker_id in expired_speakers:
                    del self.speaker_profiles[speaker_id]
                
                print(f"í™”ì í”„ë¡œí•„ {len(self.speaker_profiles)}ê°œ ë¡œë“œ ì™„ë£Œ")
                if expired_speakers:
                    print(f"ë§Œë£Œëœ í™”ì {len(expired_speakers)}ê°œ ì •ë¦¬")
                    
        except Exception as e:
            print(f"í™”ì í”„ë¡œí•„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.speaker_profiles = {}
    
    def save_speaker_profiles(self):
        """í™”ì í”„ë¡œí•„ ì €ì¥"""
        try:
            with open(config.storage.SPEAKER_PROFILES_PATH, 'wb') as f:
                pickle.dump(self.speaker_profiles, f)
            print(f"í™”ì í”„ë¡œí•„ {len(self.speaker_profiles)}ê°œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"í™”ì í”„ë¡œí•„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_speaker_embedding(self, audio_segment: np.ndarray, diar_pipeline) -> Optional[List[float]]:
        """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ í™”ì ì„ë² ë”© ì¶”ì¶œ"""
        try:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                sf.write(tmp_file.name, audio_segment, config.audio.SAMPLE_RATE)
                
                # pyannote ì„ë² ë”© ì¶”ì¶œ
                embedding_result = diar_pipeline._embedding(tmp_file.name)
                if hasattr(embedding_result, 'data') and len(embedding_result.data) > 0:
                    speaker_embedding = np.mean(embedding_result.data, axis=0)
                    os.unlink(tmp_file.name)
                    return speaker_embedding.tolist()
                
                os.unlink(tmp_file.name)
                return None
                
        except Exception as e:
            print(f"í™”ì ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def identify_speaker(self, pyannote_speaker: str, audio_segment: np.ndarray, 
                        segment_duration: float, diar_pipeline) -> str:
        """pyannote í™”ì IDë¥¼ ì‹¤ì œ í™”ì IDë¡œ ë§¤í•‘"""
        current_time = time.time()
        
        # ì´ë¯¸ ë§¤í•‘ëœ í™”ìì¸ ê²½ìš°
        if pyannote_speaker in self.session_speaker_mapping:
            mapped_speaker = self.session_speaker_mapping[pyannote_speaker]
            
            # í™”ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            if mapped_speaker in self.speaker_profiles:
                profile = self.speaker_profiles[mapped_speaker]
                profile.last_seen = current_time
                profile.total_duration += segment_duration
                profile.sample_count += 1
                
                # ìƒˆë¡œìš´ ì„ë² ë”© ì¶”ê°€ (ê°€ë”ì”©)
                if profile.sample_count % 5 == 0:
                    speaker_embedding = self.get_speaker_embedding(audio_segment, diar_pipeline)
                    if speaker_embedding:
                        profile.embeddings.append(speaker_embedding)
                        if len(profile.embeddings) > config.speaker.MAX_SPEAKER_EMBEDDINGS:
                            profile.embeddings.pop(0)
            
            return mapped_speaker
        
        # ìƒˆë¡œìš´ pyannote í™”ì - ê¸°ì¡´ í™”ìì™€ ë§¤ì¹­ ì‹œë„
        speaker_embedding = self.get_speaker_embedding(audio_segment, diar_pipeline)
        if not speaker_embedding:
            # ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ ID í• ë‹¹
            temp_speaker_id = f"Speaker_{len(self.session_speaker_mapping) + 1}"
            self.session_speaker_mapping[pyannote_speaker] = temp_speaker_id
            return temp_speaker_id
        
        best_match_speaker = None
        best_similarity = 0.0
        
        # ê¸°ì¡´ í™”ìë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
        for speaker_id, profile in self.speaker_profiles.items():
            if not profile.embeddings:
                continue
            
            similarities = []
            for stored_embedding in profile.embeddings:
                similarity = cosine_similarity([speaker_embedding], [stored_embedding])[0][0]
                similarities.append(similarity)
            
            avg_similarity = np.mean(similarities)
            if avg_similarity > best_similarity:
                best_similarity = avg_similarity
                best_match_speaker = speaker_id
        
        # ì„ê³„ê°’ ì´ìƒì˜ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ í™”ìê°€ ìˆëŠ” ê²½ìš°
        if best_match_speaker and best_similarity >= config.speaker.SIMILARITY_THRESHOLD:
            real_speaker_id = best_match_speaker
            print(f"í™”ì ì—°ê²°: {pyannote_speaker} -> {real_speaker_id} (ìœ ì‚¬ë„: {best_similarity:.3f})")
        else:
            # ìƒˆë¡œìš´ í™”ì ìƒì„±
            existing_persons = [s for s in self.speaker_profiles.keys() if s.startswith('Person_')]
            real_speaker_id = f"Person_{chr(65 + len(existing_persons))}"
            print(f"ìƒˆ í™”ì ë“±ë¡: {pyannote_speaker} -> {real_speaker_id}")
        
        # ë§¤í•‘ ì €ì¥
        self.session_speaker_mapping[pyannote_speaker] = real_speaker_id
        
        # í™”ì í”„ë¡œí•„ ìƒì„±/ì—…ë°ì´íŠ¸
        if real_speaker_id not in self.speaker_profiles:
            self.speaker_profiles[real_speaker_id] = SpeakerProfile(
                speaker_id=real_speaker_id,
                embeddings=[speaker_embedding],
                last_seen=current_time,
                total_duration=segment_duration,
                sample_count=1,
                display_name=real_speaker_id
            )
        else:
            profile = self.speaker_profiles[real_speaker_id]
            profile.embeddings.append(speaker_embedding)
            if len(profile.embeddings) > config.speaker.MAX_SPEAKER_EMBEDDINGS:
                profile.embeddings.pop(0)
            profile.last_seen = current_time
            profile.total_duration += segment_duration
            profile.sample_count += 1
        
        return real_speaker_id

class AudioProcessor(QObject):
    """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ì‹¤ì‹œê°„ STT í´ë˜ìŠ¤"""
    
    # Signals
    segment_ready = pyqtSignal(object)  # Segment
    status_update = pyqtSignal(str)
    diarization_update = pyqtSignal(list)  # List[tuple]
    
    def __init__(self):
        super().__init__()
        self.audio_queue = queue.Queue()
        self.audio_buffer = deque(maxlen=int(config.audio.BUFFER_DURATION * config.audio.SAMPLE_RATE))
        self.is_recording = False
        self.diar_segments: List[tuple] = []
        
        # PyAudio
        self.pyaudio_instance = None
        self.audio_stream = None
        
        # Models
        self.whisper_model: Optional[WhisperModel] = None
        self.diar_pipeline = None
        self.embedding_model: Optional[SentenceTransformer] = None
        
        # Speaker management
        self.speaker_manager = SpeakerManager()
        
        # Processing
        self.processing_thread = None
        self.diarization_thread = None
        self._stop_event = threading.Event()
        self._file_lock = threading.Lock()  # íŒŒì¼ ì ‘ê·¼ ë™ê¸°í™”ë¥¼ ìœ„í•œ ë½
        
        # Audio file management
        self.raw_audio_path: Optional[str] = None
        self.wave_file: Optional[wave.Wave_write] = None
        self._frames_elapsed = 0

        # ì‹œê·¸ë„ ì—°ê²°
        self.diarization_update.connect(self.on_diarization_update)

    @pyqtSlot(list)
    def on_diarization_update(self, segments: list):
        """í™”ìë¶„ë¦¬ ê²°ê³¼ ì—…ë°ì´íŠ¸"""
        self.diar_segments = segments
        
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if WhisperModel is None:
            raise RuntimeError("faster-whisper ë¯¸ì„¤ì¹˜")
        
        self.status_update.emit("ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # Whisper ëª¨ë¸ ë¡œë“œ
        try:
            self.whisper_model = WhisperModel(
                config.model.WHISPER_MODEL,
                device=config.model.WHISPER_DEVICE,
                compute_type=config.model.WHISPER_COMPUTE_TYPE
            )
            self.status_update.emit(f"Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {config.model.WHISPER_MODEL}")
        except Exception as e:
            self.status_update.emit(f"Whisper GPU ì‹¤íŒ¨ -> CPU ì¬ì‹œë„: {e}")
            self.whisper_model = WhisperModel(
                config.model.WHISPER_MODEL,
                device="cpu",
                compute_type="int8"
            )
        
        # Diarization íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        if PyannotePipeline and os.getenv(config.model.HF_TOKEN_ENV):
            try:
                self.diar_pipeline = PyannotePipeline.from_pretrained(
                    config.model.PYANNOTE_PIPELINE_NAME,
                    use_auth_token=os.getenv(config.model.HF_TOKEN_ENV)
                )
                self.status_update.emit("í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                self.status_update.emit(f"í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # Embedding ëª¨ë¸ ë¡œë“œ
        if SentenceTransformer:
            try:
                self.embedding_model = SentenceTransformer(config.model.EMBEDDING_MODEL)
                self.status_update.emit("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                self.status_update.emit(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def start_recording(self):
        """ë…¹ìŒ ì‹œì‘"""
        if pyaudio is None:
            raise RuntimeError("PyAudio ë¯¸ì„¤ì¹˜")
        
        self.initialize_models()
        self._stop_event.clear()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„
        fd, self.raw_audio_path = tempfile.mkstemp(suffix=".wav", prefix="meeting_")
        os.close(fd)
        
        self.wave_file = wave.open(self.raw_audio_path, 'wb')
        self.wave_file.setnchannels(config.audio.CHANNELS)
        self.wave_file.setsampwidth(config.audio.SAMPLE_WIDTH)
        self.wave_file.setframerate(config.audio.SAMPLE_RATE)
        
        # PyAudio ì´ˆê¸°í™”
        self.pyaudio_instance = pyaudio.PyAudio()
        self.audio_stream = self.pyaudio_instance.open(
            format=self.pyaudio_instance.get_format_from_width(config.audio.SAMPLE_WIDTH),
            channels=config.audio.CHANNELS,
            rate=config.audio.SAMPLE_RATE,
            input=True,
            frames_per_buffer=int(config.audio.SAMPLE_RATE * 0.2),
            stream_callback=self._audio_callback
        )
        
        self.is_recording = True
        self.audio_stream.start_stream()
        
        # ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)
        self.processing_thread.start()
        
        # í™”ìë¶„ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        if self.diar_pipeline:
            self.diarization_thread = threading.Thread(target=self._diarization_loop, daemon=True)
            self.diarization_thread.start()
        
        self.status_update.emit("ë…¹ìŒ ì‹œì‘ë¨")
    
    def stop_recording(self):
        """ë…¹ìŒ ì¤‘ì§€"""
        self.is_recording = False
        self._stop_event.set()
        
        try:
            if self.audio_stream:
                self.audio_stream.stop_stream()
                self.audio_stream.close()
            if self.pyaudio_instance:
                self.pyaudio_instance.terminate()
            
            # ì›¨ì´ë¸Œ íŒŒì¼ ë‹«ê¸° (ë½ ì‚¬ìš©)
            with self._file_lock:
                if self.wave_file:
                    self.wave_file.close()
                    self.wave_file = None
                
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # í™”ì í”„ë¡œí•„ ì €ì¥
        self.speaker_manager.save_speaker_profiles()
        
        self.status_update.emit("ë…¹ìŒ ì¤‘ì§€ë¨")
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """PyAudio ì½œë°±"""
        if status:
            print(f"ì˜¤ë””ì˜¤ ì½œë°± ìƒíƒœ: {status}")
        
        # ì›ë³¸ ë°ì´í„° íŒŒì¼ì— ì“°ê¸° (ë½ ì‚¬ìš©)
        with self._file_lock:
            if self.wave_file:
                self.wave_file.writeframes(in_data)
            
        # numpy arrayë¡œ ë³€í™˜ (ë¶„ì„ìš©)
        audio_data = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # ë²„í¼ì— ì¶”ê°€
        self.audio_buffer.extend(audio_data)
        self.audio_queue.put(audio_data.copy())
        self._frames_elapsed += frame_count
        
        return (None, pyaudio.paContinue)
    
    def _processing_loop(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë©”ì¸ ë£¨í”„"""
        process_buffer = []
        last_process_time = time.time()
        
        while not self._stop_event.is_set():
            try:
                # ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì§‘
                try:
                    audio_chunk = self.audio_queue.get(timeout=0.1)
                    process_buffer.extend(audio_chunk)
                except queue.Empty:
                    continue
                
                current_time = time.time()
                buffer_duration = len(process_buffer) / config.audio.SAMPLE_RATE
                
                # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ì²˜ë¦¬
                if (buffer_duration >= config.audio.PROCESS_INTERVAL or
                    current_time - last_process_time >= config.audio.PROCESS_INTERVAL):
                    
                    if len(process_buffer) > 0:
                        audio_array = np.array(process_buffer, dtype=np.float32)
                        base_time = current_time - buffer_duration
                        
                        # STT ì²˜ë¦¬
                        self._process_stt(audio_array, base_time)
                        
                        # ê²¹ì¹¨ ìœ ì§€
                        overlap_samples = int(config.audio.OVERLAP_DURATION * config.audio.SAMPLE_RATE)
                        if len(process_buffer) > overlap_samples:
                            process_buffer = process_buffer[-overlap_samples:]
                        else:
                            process_buffer = []
                        
                        last_process_time = current_time
                
            except Exception as e:
                print(f"ì²˜ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(0.1)
    
    def _process_stt(self, audio_data: np.ndarray, base_time: float):
        """STT ì²˜ë¦¬"""
        try:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp_file:
                sf.write(tmp_file.name, audio_data, config.audio.SAMPLE_RATE)
                
                # Whisper STT
                segments, info = self.whisper_model.transcribe(
                    tmp_file.name,
                    language=config.model.WHISPER_LANG,
                    vad_filter=True
                )
                
                for whisper_seg in segments:
                    if not whisper_seg.text.strip():
                        continue
                    
                    segment = Segment(
                        start=base_time + whisper_seg.start,
                        end=base_time + whisper_seg.end,
                        text=whisper_seg.text.strip(),
                        speaker_id="Unknown",
                        speaker_name="Unknown"
                    )
                    
                    # í™”ì ì‹ë³„ (í™”ìë¶„ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°)
                    if self.diar_pipeline and self.diar_segments:
                        speaker = self._find_speaker_for_segment(segment)
                        if speaker:
                            segment.speaker_name = speaker
                            segment.speaker_id = speaker
                    
                    self.segment_ready.emit(segment)
                    
        except Exception as e:
            print(f"STT ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def _find_speaker_for_segment(self, segment: Segment) -> Optional[str]:
        """ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œê°„ê³¼ ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” í™”ìë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        max_overlap = 0
        best_speaker = None
        
        for diar_start, diar_end, speaker in self.diar_segments:
            overlap_start = max(segment.start, diar_start)
            overlap_end = min(segment.end, diar_end)
            overlap_duration = overlap_end - overlap_start
            
            if overlap_duration > max_overlap:
                max_overlap = overlap_duration
                best_speaker = speaker
        
        # ìµœì†Œ ê²¹ì¹¨ ì‹œê°„ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ í™”ì í• ë‹¹ (ì˜µì…˜)
        segment_duration = segment.end - segment.start
        if best_speaker and max_overlap / segment_duration > 0.5: # 50% ì´ìƒ ê²¹ì¹  ë•Œ
            return best_speaker
            
        return None
    
    def _diarization_loop(self):
        """í™”ìë¶„ë¦¬ ì²˜ë¦¬ ë£¨í”„ (ì£¼ê¸°ì )"""
        while not self._stop_event.is_set():
            time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì‹¤í–‰
            
            if not self.raw_audio_path or not self.is_recording:
                continue

            diar_path = None
            try:
                with self._file_lock:
                    if self.wave_file:
                        # íŒŒì¼ì„ ë‹«ì•„ ì½ê¸° ê°€ëŠ¥ ìƒíƒœë¡œ ë§Œë“¦
                        self.wave_file.close()
                        diar_path = self.raw_audio_path
                    else:
                        continue

                # í™”ìë¶„ë¦¬ ì‹¤í–‰ (ë½ ì™¸ë¶€ì—ì„œ)
                if diar_path and os.path.exists(diar_path):
                    self.status_update.emit("í™”ìë¶„ë¦¬ ì‹¤í–‰ ì¤‘...")
                    diar_result = self.diar_pipeline(diar_path)
                    segments = []
                    for turn, _, speaker in diar_result.itertracks(yield_label=True):
                        segments.append((turn.start, turn.end, speaker))
                    
                    if segments:
                        self.diarization_update.emit(segments)
                        self.status_update.emit(f"í™”ìë¶„ë¦¬ ì—…ë°ì´íŠ¸: {len(segments)}ê°œ êµ¬ê°„")

            except Exception as e:
                print(f"í™”ìë¶„ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            finally:
                # íŒŒì¼ì„ ë‹¤ì‹œ append ëª¨ë“œë¡œ ì—´ê¸° (ë½ ë‚´ë¶€ì—ì„œ)
                with self._file_lock:
                    if self.is_recording and self.raw_audio_path and not self.wave_file:
                        try:
                            self.wave_file = wave.open(self.raw_audio_path, 'ab')
                        except Exception as e:
                            print(f"ì›¨ì´ë¸Œ íŒŒì¼ ë‹¤ì‹œ ì—´ê¸° ì‹¤íŒ¨: {e}")

# ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import time
    import sys
    from datetime import datetime
    
    print("=" * 50)
    print("Audio Processor Module Test")
    print("=" * 50)
    
    # ì˜ì¡´ì„± ì²´í¬
    print("ğŸ“¦ Dependency Check:")
    print(f"  - PyAudio: {'âœ… Available' if pyaudio else 'âŒ Not available'}")
    print(f"  - Faster-Whisper: {'âœ… Available' if WhisperModel else 'âŒ Not available'}")
    print(f"  - Pyannote: {'âœ… Available' if PyannotePipeline else 'âŒ Not available'}")
    print(f"  - SentenceTransformers: {'âœ… Available' if SentenceTransformer else 'âŒ Not available'}")
    
    # SpeakerManager í…ŒìŠ¤íŠ¸
    print("\nğŸ‘¤ SpeakerManager Test:")
    try:
        speaker_manager = SpeakerManager()
        print(f"  - Speaker profiles loaded: {len(speaker_manager.speaker_profiles)}")
        print(f"  - Session mappings: {len(speaker_manager.session_speaker_mapping)}")
        
        # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„°ë¡œ ì„ë² ë”© í…ŒìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŒ)
        print("  - Embedding extraction test: Simulation only")
        print("    (Real test requires actual audio data and diarization pipeline)")
        
    except Exception as e:
        print(f"  âŒ SpeakerManager test failed: {e}")
    
    # AudioProcessor ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    print("\nğŸ¤ AudioProcessor Initialization Test:")
    try:
        from PyQt6.QtCore import QCoreApplication
        
        # Qt ì• í”Œë¦¬ì¼€ì´ì…˜ í•„ìš” (QObject ìƒì† ë•Œë¬¸ì—)
        app = QCoreApplication(sys.argv) if not QCoreApplication.instance() else QCoreApplication.instance()
        
        processor = AudioProcessor()
        print("  âœ… AudioProcessor created successfully")
        print(f"  - Recording status: {processor.is_recording}")
        print(f"  - Audio queue size: {processor.audio_queue.qsize()}")
        print(f"  - Buffer max length: {processor.audio_buffer.maxlen}")
        
        # ì‹œê·¸ë„ ì—°ê²° í…ŒìŠ¤íŠ¸
        def test_signal_handler(message):
            print(f"  ğŸ“¡ Signal received: {message}")
        
        processor.status_update.connect(test_signal_handler)
        processor.status_update.emit("Test signal emission")
        
    except ImportError:
        print("  âš ï¸ PyQt6 not available - creating mock processor")
        print("  (Full test requires PyQt6 installation)")
    except Exception as e:
        print(f"  âŒ AudioProcessor test failed: {e}")
    
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    print(f"\nâš™ï¸ Configuration Test:")
    print(f"  - Sample Rate: {config.audio.SAMPLE_RATE} Hz")
    print(f"  - Channels: {config.audio.CHANNELS}")
    print(f"  - Buffer Duration: {config.audio.BUFFER_DURATION}s")
    print(f"  - Process Interval: {config.audio.PROCESS_INTERVAL}s")
    print(f"  - Chunk Size: {config.audio.CHUNK_SIZE}")
    
    # ì˜¤ë””ì˜¤ ì¥ì¹˜ ëª©ë¡ í…ŒìŠ¤íŠ¸ (PyAudio ì‚¬ìš© ê°€ëŠ¥ì‹œ)
    if pyaudio:
        print(f"\nğŸ”Š Available Audio Devices:")
        try:
            p = pyaudio.PyAudio()
            device_count = p.get_device_count()
            print(f"  Total devices: {device_count}")
            
            for i in range(min(5, device_count)):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                info = p.get_device_info_by_index(i)
                print(f"  {i}: {info['name']} (inputs: {info['maxInputChannels']})")
            
            p.terminate()
            
        except Exception as e:
            print(f"  âŒ Audio device enumeration failed: {e}")
    
    # ëª¨ë¸ ì„¤ì • í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¤– Model Configuration Test:")
    print(f"  - Whisper Model: {config.model.WHISPER_MODEL}")
    print(f"  - Device: {config.model.WHISPER_DEVICE}")
    print(f"  - Compute Type: {config.model.WHISPER_COMPUTE_TYPE}")
    print(f"  - Language: {config.model.WHISPER_LANG}")
    
    # ì„ì‹œ íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“ File Operations Test:")
    try:
        import tempfile
        import soundfile as sf
        import numpy as np
        
        # ì„ì‹œ WAV íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            # 1ì´ˆê°„ì˜ ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„±
            sample_rate = config.audio.SAMPLE_RATE
            duration = 1.0
            samples = int(sample_rate * duration)
            
            # ì‚¬ì¸íŒŒ ìƒì„± (440Hz)
            t = np.linspace(0, duration, samples)
            audio_data = 0.5 * np.sin(2 * np.pi * 440 * t)
            
            sf.write(tmp_file.name, audio_data, sample_rate)
            print(f"  âœ… Temporary WAV file created: {tmp_file.name}")
            
            # íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸
            read_data, read_sr = sf.read(tmp_file.name)
            print(f"  âœ… File read successfully: {len(read_data)} samples at {read_sr}Hz")
            
            # íŒŒì¼ ì‚­ì œ
            import os
            os.unlink(tmp_file.name)
            print(f"  âœ… Temporary file cleaned up")
            
    except Exception as e:
        print(f"  âŒ File operations test failed: {e}")
    
    # ì‹¤ì œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë°ëª¨ (PyAudioì™€ Whisperê°€ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if pyaudio and WhisperModel:
        print(f"\nğŸ™ï¸ Audio Processing Demo:")
        response = input("  Start 5-second recording demo? (y/N): ").strip().lower()
        
        if response == 'y':
            try:
                print("  Preparing audio recording...")
                
                # ê°„ë‹¨í•œ ë…¹ìŒ í…ŒìŠ¤íŠ¸
                p = pyaudio.PyAudio()
                
                stream = p.open(
                    format=p.get_format_from_width(config.audio.SAMPLE_WIDTH),
                    channels=config.audio.CHANNELS,
                    rate=config.audio.SAMPLE_RATE,
                    input=True,
                    frames_per_buffer=1024
                )
                
                print("  ğŸ”´ Recording for 5 seconds... Speak now!")
                frames = []
                
                for _ in range(0, int(config.audio.SAMPLE_RATE / 1024 * 5)):
                    data = stream.read(1024)
                    frames.append(data)
                
                print("  â¹ï¸ Recording finished")
                
                stream.stop_stream()
                stream.close()
                p.terminate()
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    with wave.open(tmp_file.name, 'wb') as wf:
                        wf.setnchannels(config.audio.CHANNELS)
                        wf.setsampwidth(p.get_sample_size(p.get_format_from_width(config.audio.SAMPLE_WIDTH)))
                        wf.setframerate(config.audio.SAMPLE_RATE)
                        wf.writeframes(b''.join(frames))
                    
                    print(f"  ğŸ’¾ Audio saved to: {tmp_file.name}")
                    
                    # Whisper í…ŒìŠ¤íŠ¸
                    print("  ğŸ§  Loading Whisper model...")
                    model = WhisperModel(
                        config.model.WHISPER_MODEL,
                        device="cpu",  # ì•ˆì „ì„ ìœ„í•´ CPU ì‚¬ìš©
                        compute_type="int8"
                    )
                    
                    print("  ğŸ¯ Transcribing audio...")
                    segments, info = model.transcribe(tmp_file.name, language=config.model.WHISPER_LANG)
                    
                    print("  ğŸ“ Transcription results:")
                    for segment in segments:
                        print(f"    [{segment.start:.1f}s - {segment.end:.1f}s] {segment.text}")
                    
                    # ì •ë¦¬
                    import os
                    os.unlink(tmp_file.name)
                    print("  âœ… Demo completed and cleaned up")
                
            except Exception as e:
                print(f"  âŒ Recording demo failed: {e}")
        else:
            print("  Demo skipped")
    else:
        print(f"\nâš ï¸ Audio Processing Demo:")
        print("  Demo requires both PyAudio and Faster-Whisper")
        missing = []
        if not pyaudio:
            missing.append("PyAudio")
        if not WhisperModel:
            missing.append("Faster-Whisper")
        print(f"  Missing: {', '.join(missing)}")
    
    print(f"\n" + "=" * 50)
    print("Audio Processor Module Test Complete!")
    
    if '--interactive' in sys.argv:
        print("\nInteractive mode - press Enter to exit")
        input()